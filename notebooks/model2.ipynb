{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "# import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   popularity  duration_ms  explicit  danceability  energy  key  loudness   \n0          34       247707         0         0.491   0.606    7   -10.172  \\\n1          34       247707         0         0.491   0.606    7   -10.172   \n2          34       247707         0         0.491   0.606    7   -10.172   \n3          35       140067         0         0.449   0.749    9    -8.585   \n4          35       140067         0         0.449   0.749    9    -8.585   \n\n   speechiness  acousticness  instrumentalness  liveness  valence    tempo   \n0       0.0377       0.00327          0.000008     0.341    0.669  123.025  \\\n1       0.0377       0.00327          0.000008     0.341    0.669  123.025   \n2       0.0377       0.00327          0.000008     0.341    0.669  123.025   \n3       0.0775       0.01000          0.000000     0.391    0.448  106.861   \n4       0.0775       0.01000          0.000000     0.391    0.448  106.861   \n\n                                    favourite_genres   \n0                   [permanent wave, mandopop, funk]  \\\n1                    [filmi, regional mexican, folk]   \n2  [psychedelic rock, country rock, rock en espanol]   \n3  [psychedelic rock, country rock, rock en espanol]   \n4  [psychedelic rock, country rock, rock en espanol]   \n\n                                              genres  skipped   \n0  [album rock, art rock, classic rock, folk rock...    False  \\\n1  [album rock, art rock, classic rock, folk rock...    False   \n2  [album rock, art rock, classic rock, folk rock...    False   \n3  [album rock, art rock, classic rock, folk rock...    False   \n4  [album rock, art rock, classic rock, folk rock...    False   \n\n   number_of_matching_genres  \n0                          0  \n1                          0  \n2                          1  \n3                          1  \n4                          1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>popularity</th>\n      <th>duration_ms</th>\n      <th>explicit</th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>key</th>\n      <th>loudness</th>\n      <th>speechiness</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>tempo</th>\n      <th>favourite_genres</th>\n      <th>genres</th>\n      <th>skipped</th>\n      <th>number_of_matching_genres</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34</td>\n      <td>247707</td>\n      <td>0</td>\n      <td>0.491</td>\n      <td>0.606</td>\n      <td>7</td>\n      <td>-10.172</td>\n      <td>0.0377</td>\n      <td>0.00327</td>\n      <td>0.000008</td>\n      <td>0.341</td>\n      <td>0.669</td>\n      <td>123.025</td>\n      <td>[permanent wave, mandopop, funk]</td>\n      <td>[album rock, art rock, classic rock, folk rock...</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34</td>\n      <td>247707</td>\n      <td>0</td>\n      <td>0.491</td>\n      <td>0.606</td>\n      <td>7</td>\n      <td>-10.172</td>\n      <td>0.0377</td>\n      <td>0.00327</td>\n      <td>0.000008</td>\n      <td>0.341</td>\n      <td>0.669</td>\n      <td>123.025</td>\n      <td>[filmi, regional mexican, folk]</td>\n      <td>[album rock, art rock, classic rock, folk rock...</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34</td>\n      <td>247707</td>\n      <td>0</td>\n      <td>0.491</td>\n      <td>0.606</td>\n      <td>7</td>\n      <td>-10.172</td>\n      <td>0.0377</td>\n      <td>0.00327</td>\n      <td>0.000008</td>\n      <td>0.341</td>\n      <td>0.669</td>\n      <td>123.025</td>\n      <td>[psychedelic rock, country rock, rock en espanol]</td>\n      <td>[album rock, art rock, classic rock, folk rock...</td>\n      <td>False</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35</td>\n      <td>140067</td>\n      <td>0</td>\n      <td>0.449</td>\n      <td>0.749</td>\n      <td>9</td>\n      <td>-8.585</td>\n      <td>0.0775</td>\n      <td>0.01000</td>\n      <td>0.000000</td>\n      <td>0.391</td>\n      <td>0.448</td>\n      <td>106.861</td>\n      <td>[psychedelic rock, country rock, rock en espanol]</td>\n      <td>[album rock, art rock, classic rock, folk rock...</td>\n      <td>False</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>35</td>\n      <td>140067</td>\n      <td>0</td>\n      <td>0.449</td>\n      <td>0.749</td>\n      <td>9</td>\n      <td>-8.585</td>\n      <td>0.0775</td>\n      <td>0.01000</td>\n      <td>0.000000</td>\n      <td>0.391</td>\n      <td>0.448</td>\n      <td>106.861</td>\n      <td>[psychedelic rock, country rock, rock en espanol]</td>\n      <td>[album rock, art rock, classic rock, folk rock...</td>\n      <td>False</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data_path = '../data/merged_data.jsonl'\n",
    "data = pd.read_json(merged_data_path, lines=True)\n",
    "\n",
    "# data = data.drop(\n",
    "#     columns=[\"release_date\", \"key\", \"loudness\",\n",
    "#              \"explicit\", \"popularity\", \"duration_ms\", \"danceability\", \"energy\", \"speechiness\",\n",
    "#              \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"])\n",
    "\n",
    "data = data.drop(\n",
    "    columns=[\"release_date\", \"name\"])\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_favourite_genres = set()\n",
    "# for genres in data['favourite_genres']:\n",
    "#   for genre in genres:\n",
    "#     unique_favourite_genres.add(genre)\n",
    "#\n",
    "# unique_genres = set()\n",
    "# for genres in data['genres']:\n",
    "#   for genre in genres:\n",
    "#     unique_genres.add(genre)\n",
    "#\n",
    "# # show all common genres between all favourite_genres and genres\n",
    "# print(\"unique_favourite_genres\", len(unique_favourite_genres))\n",
    "# print(\"unique_genres\", len(unique_genres))\n",
    "# print(\"common genres\", unique_favourite_genres.intersection(unique_genres))\n",
    "#\n",
    "# # remove genres that are not in favourite_genres\n",
    "# data['genres'] = data['genres'].apply(\n",
    "#     lambda x: [genre for genre in x if genre in unique_favourite_genres])\n",
    "\n",
    "# TODO remove ??? moze niepotrzebne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9881, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   popularity  popularity_normalized  duration_ms  duration_ms_normalized   \n0          34               0.354167       247707                0.092836  \\\n1          34               0.354167       247707                0.092836   \n2          34               0.354167       247707                0.092836   \n3          35               0.364583       140067                0.046724   \n4          35               0.364583       140067                0.046724   \n\n   skipped  \n0    False  \n1    False  \n2    False  \n3    False  \n4    False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>popularity</th>\n      <th>popularity_normalized</th>\n      <th>duration_ms</th>\n      <th>duration_ms_normalized</th>\n      <th>skipped</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34</td>\n      <td>0.354167</td>\n      <td>247707</td>\n      <td>0.092836</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34</td>\n      <td>0.354167</td>\n      <td>247707</td>\n      <td>0.092836</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34</td>\n      <td>0.354167</td>\n      <td>247707</td>\n      <td>0.092836</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35</td>\n      <td>0.364583</td>\n      <td>140067</td>\n      <td>0.046724</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>35</td>\n      <td>0.364583</td>\n      <td>140067</td>\n      <td>0.046724</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine genres and favourite_genres\n",
    "all_genres = list(data['favourite_genres'] + data['genres'])\n",
    "\n",
    "# One-hot encode the genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(all_genres)\n",
    "\n",
    "encoded_all_genres = mlb.fit_transform(all_genres)\n",
    "\n",
    "# Split encoded_genres into genres and favourite_genres\n",
    "# encoded_favourite_genres = encoded_all_genres[:, :len(data['favourite_genres'][0])]\n",
    "# encoded_genres = encoded_all_genres[:, len(data['favourite_genres'][0]):]\n",
    "\n",
    "encoded_favourite_genres = mlb.transform(data['favourite_genres'])\n",
    "encoded_genres = mlb.transform(data['genres'])\n",
    "\n",
    "popularity_normalized = data['popularity'].values.reshape(-1, 1)\n",
    "popularity_normalized = (popularity_normalized - np.min(popularity_normalized)) / (np.max(popularity_normalized) - np.min(popularity_normalized))\n",
    "\n",
    "duration_ms_normalized = data['duration_ms'].values.reshape(-1, 1)\n",
    "duration_ms_normalized = (duration_ms_normalized - np.min(duration_ms_normalized)) / (np.max(duration_ms_normalized) - np.min(duration_ms_normalized))\n",
    "\n",
    "print(popularity_normalized.shape)\n",
    "\n",
    "# create data frame from data genres, data favourite_genres, encoded genres, encoded favourite_genres\n",
    "df = pd.DataFrame(\n",
    "  data={'popularity': data['popularity'], 'popularity_normalized': popularity_normalized.reshape(-1),\n",
    "        'duration_ms': data['duration_ms'], 'duration_ms_normalized': duration_ms_normalized.reshape(-1),\n",
    "        'skipped': data['skipped']})\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train not skipped 1261\n",
      "Y_train skipped 716\n",
      "Y_train skipped % 0.09058704453441295\n"
     ]
    }
   ],
   "source": [
    "# TODO uzywac keras tokenizer?\n",
    "\n",
    "# Concatenate the one-hot encoded columns\n",
    "X = np.concatenate([encoded_favourite_genres, encoded_genres, popularity_normalized, duration_ms_normalized], axis=1)\n",
    "# X = np.concatenate([popularity_normalized, duration_ms_normalized], axis=1)\n",
    "\n",
    "# Extract the labels\n",
    "y = data['skipped'].astype(int).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# count skipped and not skipped songs in X_train\n",
    "print(\"Y_train not skipped\", np.count_nonzero(y_test == 0))\n",
    "print(\"Y_train skipped\", np.count_nonzero(y_test == 1))\n",
    "print(\"Y_train skipped %\", np.count_nonzero(y_test == 1) / len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(3000 * 2, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(3000, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  # model.add(Dense(1000, activation='relu'))\n",
    "  # model.add(Dropout(0.5))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  # model.compile(\n",
    "  #     optimizer=keras.optimizers.Adam(hp.Choice('learning_date', values=[0.5, 0.1, 0.01])),\n",
    "  #     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  model.compile(\n",
    "      optimizer=Adam(),\n",
    "      loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "247/247 - 5s - loss: 0.6159 - accuracy: 0.6645 - val_loss: 0.5966 - val_accuracy: 0.6793 - 5s/epoch - 20ms/step\n",
      "Epoch 2/50\n",
      "247/247 - 4s - loss: 0.5600 - accuracy: 0.7010 - val_loss: 0.5959 - val_accuracy: 0.6823 - 4s/epoch - 16ms/step\n",
      "Epoch 3/50\n",
      "247/247 - 4s - loss: 0.5197 - accuracy: 0.7227 - val_loss: 0.6066 - val_accuracy: 0.6783 - 4s/epoch - 16ms/step\n",
      "Epoch 4/50\n",
      "247/247 - 4s - loss: 0.4814 - accuracy: 0.7530 - val_loss: 0.6707 - val_accuracy: 0.6879 - 4s/epoch - 16ms/step\n",
      "Epoch 5/50\n",
      "247/247 - 4s - loss: 0.4447 - accuracy: 0.7839 - val_loss: 0.6750 - val_accuracy: 0.6697 - 4s/epoch - 16ms/step\n",
      "Epoch 6/50\n",
      "247/247 - 4s - loss: 0.4064 - accuracy: 0.8004 - val_loss: 0.7286 - val_accuracy: 0.6732 - 4s/epoch - 16ms/step\n",
      "Epoch 7/50\n",
      "247/247 - 4s - loss: 0.3802 - accuracy: 0.8106 - val_loss: 0.7958 - val_accuracy: 0.6763 - 4s/epoch - 16ms/step\n",
      "Epoch 8/50\n",
      "247/247 - 4s - loss: 0.3548 - accuracy: 0.8240 - val_loss: 0.7770 - val_accuracy: 0.6525 - 4s/epoch - 15ms/step\n",
      "Epoch 9/50\n",
      "247/247 - 4s - loss: 0.3285 - accuracy: 0.8369 - val_loss: 0.8431 - val_accuracy: 0.6631 - 4s/epoch - 15ms/step\n",
      "Epoch 10/50\n",
      "247/247 - 4s - loss: 0.3131 - accuracy: 0.8446 - val_loss: 0.8995 - val_accuracy: 0.6662 - 4s/epoch - 16ms/step\n",
      "Epoch 11/50\n",
      "247/247 - 4s - loss: 0.2973 - accuracy: 0.8493 - val_loss: 1.0119 - val_accuracy: 0.6601 - 4s/epoch - 17ms/step\n",
      "Epoch 12/50\n",
      "247/247 - 4s - loss: 0.2883 - accuracy: 0.8592 - val_loss: 0.9533 - val_accuracy: 0.6641 - 4s/epoch - 16ms/step\n",
      "Epoch 13/50\n",
      "247/247 - 4s - loss: 0.2725 - accuracy: 0.8634 - val_loss: 1.1108 - val_accuracy: 0.6560 - 4s/epoch - 16ms/step\n",
      "Epoch 14/50\n",
      "247/247 - 4s - loss: 0.2656 - accuracy: 0.8649 - val_loss: 1.1347 - val_accuracy: 0.6560 - 4s/epoch - 17ms/step\n",
      "Epoch 15/50\n",
      "247/247 - 4s - loss: 0.2610 - accuracy: 0.8645 - val_loss: 1.1013 - val_accuracy: 0.6464 - 4s/epoch - 17ms/step\n",
      "Epoch 16/50\n",
      "247/247 - 4s - loss: 0.2547 - accuracy: 0.8664 - val_loss: 1.1059 - val_accuracy: 0.6606 - 4s/epoch - 17ms/step\n",
      "Epoch 17/50\n",
      "247/247 - 4s - loss: 0.2466 - accuracy: 0.8677 - val_loss: 1.1844 - val_accuracy: 0.6571 - 4s/epoch - 16ms/step\n",
      "Epoch 18/50\n",
      "247/247 - 4s - loss: 0.2373 - accuracy: 0.8753 - val_loss: 1.1977 - val_accuracy: 0.6495 - 4s/epoch - 17ms/step\n",
      "Epoch 19/50\n",
      "247/247 - 4s - loss: 0.2344 - accuracy: 0.8797 - val_loss: 1.3184 - val_accuracy: 0.6586 - 4s/epoch - 17ms/step\n",
      "Epoch 20/50\n",
      "247/247 - 4s - loss: 0.2324 - accuracy: 0.8774 - val_loss: 1.2655 - val_accuracy: 0.6520 - 4s/epoch - 17ms/step\n",
      "Epoch 21/50\n",
      "247/247 - 4s - loss: 0.2264 - accuracy: 0.8793 - val_loss: 1.3371 - val_accuracy: 0.6409 - 4s/epoch - 16ms/step\n",
      "Epoch 22/50\n",
      "247/247 - 4s - loss: 0.2246 - accuracy: 0.8842 - val_loss: 1.4479 - val_accuracy: 0.6525 - 4s/epoch - 16ms/step\n",
      "Epoch 23/50\n",
      "247/247 - 4s - loss: 0.2174 - accuracy: 0.8851 - val_loss: 1.3663 - val_accuracy: 0.6606 - 4s/epoch - 16ms/step\n",
      "Epoch 24/50\n",
      "247/247 - 5s - loss: 0.2162 - accuracy: 0.8836 - val_loss: 1.4437 - val_accuracy: 0.6495 - 5s/epoch - 19ms/step\n",
      "Epoch 25/50\n",
      "247/247 - 4s - loss: 0.2188 - accuracy: 0.8841 - val_loss: 1.4015 - val_accuracy: 0.6449 - 4s/epoch - 18ms/step\n",
      "Epoch 26/50\n",
      "247/247 - 5s - loss: 0.2096 - accuracy: 0.8864 - val_loss: 1.5210 - val_accuracy: 0.6474 - 5s/epoch - 18ms/step\n",
      "Epoch 27/50\n",
      "247/247 - 5s - loss: 0.2088 - accuracy: 0.8873 - val_loss: 1.4736 - val_accuracy: 0.6535 - 5s/epoch - 19ms/step\n",
      "Epoch 28/50\n",
      "247/247 - 4s - loss: 0.2082 - accuracy: 0.8899 - val_loss: 1.4439 - val_accuracy: 0.6621 - 4s/epoch - 18ms/step\n",
      "Epoch 29/50\n",
      "247/247 - 4s - loss: 0.2100 - accuracy: 0.8856 - val_loss: 1.4202 - val_accuracy: 0.6368 - 4s/epoch - 15ms/step\n",
      "Epoch 30/50\n",
      "247/247 - 5s - loss: 0.2078 - accuracy: 0.8877 - val_loss: 1.5350 - val_accuracy: 0.6464 - 5s/epoch - 19ms/step\n",
      "Epoch 31/50\n",
      "247/247 - 5s - loss: 0.2003 - accuracy: 0.8907 - val_loss: 1.7066 - val_accuracy: 0.6520 - 5s/epoch - 19ms/step\n",
      "Epoch 32/50\n",
      "247/247 - 4s - loss: 0.2001 - accuracy: 0.8875 - val_loss: 1.5666 - val_accuracy: 0.6520 - 4s/epoch - 16ms/step\n",
      "Epoch 33/50\n",
      "247/247 - 4s - loss: 0.2007 - accuracy: 0.8937 - val_loss: 1.4998 - val_accuracy: 0.6515 - 4s/epoch - 15ms/step\n",
      "Epoch 34/50\n",
      "247/247 - 4s - loss: 0.1972 - accuracy: 0.8939 - val_loss: 1.6326 - val_accuracy: 0.6672 - 4s/epoch - 16ms/step\n",
      "Epoch 35/50\n",
      "247/247 - 4s - loss: 0.1974 - accuracy: 0.8925 - val_loss: 1.5912 - val_accuracy: 0.6646 - 4s/epoch - 16ms/step\n",
      "Epoch 36/50\n",
      "247/247 - 4s - loss: 0.1949 - accuracy: 0.8940 - val_loss: 1.5875 - val_accuracy: 0.6434 - 4s/epoch - 18ms/step\n",
      "Epoch 37/50\n",
      "247/247 - 4s - loss: 0.1949 - accuracy: 0.8951 - val_loss: 1.6571 - val_accuracy: 0.6535 - 4s/epoch - 17ms/step\n",
      "Epoch 38/50\n",
      "247/247 - 5s - loss: 0.1904 - accuracy: 0.8947 - val_loss: 1.6610 - val_accuracy: 0.6495 - 5s/epoch - 19ms/step\n",
      "Epoch 39/50\n",
      "247/247 - 4s - loss: 0.1922 - accuracy: 0.8945 - val_loss: 1.6682 - val_accuracy: 0.6550 - 4s/epoch - 18ms/step\n",
      "Epoch 40/50\n",
      "247/247 - 5s - loss: 0.1922 - accuracy: 0.8989 - val_loss: 1.7321 - val_accuracy: 0.6596 - 5s/epoch - 18ms/step\n",
      "Epoch 41/50\n",
      "247/247 - 5s - loss: 0.1922 - accuracy: 0.8957 - val_loss: 1.7117 - val_accuracy: 0.6550 - 5s/epoch - 19ms/step\n",
      "Epoch 42/50\n",
      "247/247 - 4s - loss: 0.1924 - accuracy: 0.8970 - val_loss: 1.6192 - val_accuracy: 0.6550 - 4s/epoch - 17ms/step\n",
      "Epoch 43/50\n",
      "247/247 - 4s - loss: 0.1873 - accuracy: 0.8970 - val_loss: 1.6345 - val_accuracy: 0.6535 - 4s/epoch - 17ms/step\n",
      "Epoch 44/50\n",
      "247/247 - 4s - loss: 0.1882 - accuracy: 0.9001 - val_loss: 1.7663 - val_accuracy: 0.6520 - 4s/epoch - 16ms/step\n",
      "Epoch 45/50\n",
      "247/247 - 4s - loss: 0.1831 - accuracy: 0.8947 - val_loss: 1.7126 - val_accuracy: 0.6571 - 4s/epoch - 16ms/step\n",
      "Epoch 46/50\n",
      "247/247 - 4s - loss: 0.1840 - accuracy: 0.8980 - val_loss: 1.6833 - val_accuracy: 0.6601 - 4s/epoch - 18ms/step\n",
      "Epoch 47/50\n",
      "247/247 - 4s - loss: 0.1858 - accuracy: 0.8957 - val_loss: 1.6822 - val_accuracy: 0.6454 - 4s/epoch - 16ms/step\n",
      "Epoch 48/50\n",
      "247/247 - 4s - loss: 0.1821 - accuracy: 0.8974 - val_loss: 1.9227 - val_accuracy: 0.6596 - 4s/epoch - 15ms/step\n",
      "Epoch 49/50\n",
      "247/247 - 4s - loss: 0.1789 - accuracy: 0.8997 - val_loss: 1.9826 - val_accuracy: 0.6555 - 4s/epoch - 17ms/step\n",
      "Epoch 50/50\n",
      "247/247 - 4s - loss: 0.1837 - accuracy: 0.8988 - val_loss: 1.7135 - val_accuracy: 0.6485 - 4s/epoch - 15ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "model = build_model(None)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# tuner = keras_tuner.tuners.Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',\n",
    "#     max_epochs=50,\n",
    "#     max_trials=10,\n",
    "#     executions_per_trial=2,\n",
    "#     directory='my_dir')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.search(\n",
    "#     (X_train, y_train),\n",
    "#     validation_data=(X_test, y_test),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 - 0s - loss: 1.7135 - accuracy: 0.6485 - 280ms/epoch - 5ms/step\n",
      "Test set accuracy: 0.6484572291374207\n",
      "TEST\n",
      "62/62 [==============================] - 0s 4ms/step\n",
      "Accuracy: 0.648457258472433\n",
      "Confusion matrix:\n",
      " [[907 354]\n",
      " [341 375]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.72      1261\n",
      "           1       0.51      0.52      0.52       716\n",
      "\n",
      "    accuracy                           0.65      1977\n",
      "   macro avg       0.62      0.62      0.62      1977\n",
      "weighted avg       0.65      0.65      0.65      1977\n",
      "\n",
      "TRAIN\n",
      "247/247 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.9151062753036437\n",
      "Confusion matrix:\n",
      " [[4618  381]\n",
      " [ 290 2615]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      4999\n",
      "           1       0.87      0.90      0.89      2905\n",
      "\n",
      "    accuracy                           0.92      7904\n",
      "   macro avg       0.91      0.91      0.91      7904\n",
      "weighted avg       0.92      0.92      0.92      7904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "\n",
    "print(\"TEST\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_classes))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_classes))\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred_classes))\n",
    "\n",
    "print(\"TRAIN\")\n",
    "y_pred = model.predict(X_train)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred_classes))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_train, y_pred_classes))\n",
    "print(\"Classification report:\\n\", classification_report(y_train, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "#\n",
    "# # Print the predicted and actual labels\n",
    "# print(\"Predicted labels:\", y_pred_classes.flatten())\n",
    "# print(\"Actual labels:\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test with new data\n",
    "#\n",
    "# new_data = [\n",
    "#   ([\"dominican pop\", \"merengue\", \"merengue tipico\", \"tropical\"],\n",
    "#    [\"blues rock\", \"country rock\", \"lounge\"])\n",
    "# ]\n",
    "#\n",
    "# labels = [\n",
    "#\n",
    "# ]\n",
    "#\n",
    "# new_df = pd.DataFrame(new_data, columns=[\"genres\", \"favourite_genres\"])\n",
    "#\n",
    "# # Combine genres and favourite_genres\n",
    "# all_new_genres = list(new_df['genres'] + new_df['favourite_genres'])\n",
    "#\n",
    "# # One-hot encode the genres using the previously fit MultiLabelBinarizer (mlb)\n",
    "# encoded_new_genres = mlb.transform(all_new_genres)\n",
    "#\n",
    "# # Split encoded_new_genres into genres and favourite_genres\n",
    "# encoded_new_genres1 = encoded_new_genres[:, :len(new_df['genres'][0])]\n",
    "# encoded_new_genres2 = encoded_new_genres[:, len(new_df['genres'][0]):]\n",
    "#\n",
    "# # Concatenate the one-hot encoded columns\n",
    "# X_new = np.concatenate([encoded_new_genres1, encoded_new_genres2], axis=1)\n",
    "#\n",
    "# y_new_pred = model.predict(X_new)\n",
    "# y_new_pred_classes = (y_new_pred > 0.5).astype(int)\n",
    "#\n",
    "# # Print the predicted labels\n",
    "# print(\"Predicted labels:\", y_new_pred_classes.flatten())\n",
    "# print(\"Actual labels:\", labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (IUM)",
   "language": "python",
   "name": "ium"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
