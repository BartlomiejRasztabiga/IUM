{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "# import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "   user_id                track_id  session_id   \n0      101  2PmGtDUyJIpYBEtI1hQIVp         124  \\\n1      103  2PmGtDUyJIpYBEtI1hQIVp         151   \n2      132  2PmGtDUyJIpYBEtI1hQIVp         544   \n3      132  5yxYokipsWlpDCt4Th4VVc         534   \n4      132  5yxYokipsWlpDCt4Th4VVc         547   \n\n                                    favourite_genres   \n0                   [permanent wave, mandopop, funk]  \\\n1                    [filmi, regional mexican, folk]   \n2  [psychedelic rock, country rock, rock en espanol]   \n3  [psychedelic rock, country rock, rock en espanol]   \n4  [psychedelic rock, country rock, rock en espanol]   \n\n                                              genres  skipped  \n0  [album rock, art rock, classic rock, folk rock...    False  \n1  [album rock, art rock, classic rock, folk rock...    False  \n2  [album rock, art rock, classic rock, folk rock...    False  \n3  [album rock, art rock, classic rock, folk rock...    False  \n4  [album rock, art rock, classic rock, folk rock...    False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>track_id</th>\n      <th>session_id</th>\n      <th>favourite_genres</th>\n      <th>genres</th>\n      <th>skipped</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>101</td>\n      <td>2PmGtDUyJIpYBEtI1hQIVp</td>\n      <td>124</td>\n      <td>[permanent wave, mandopop, funk]</td>\n      <td>[album rock, art rock, classic rock, folk rock...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>103</td>\n      <td>2PmGtDUyJIpYBEtI1hQIVp</td>\n      <td>151</td>\n      <td>[filmi, regional mexican, folk]</td>\n      <td>[album rock, art rock, classic rock, folk rock...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>132</td>\n      <td>2PmGtDUyJIpYBEtI1hQIVp</td>\n      <td>544</td>\n      <td>[psychedelic rock, country rock, rock en espanol]</td>\n      <td>[album rock, art rock, classic rock, folk rock...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>132</td>\n      <td>5yxYokipsWlpDCt4Th4VVc</td>\n      <td>534</td>\n      <td>[psychedelic rock, country rock, rock en espanol]</td>\n      <td>[album rock, art rock, classic rock, folk rock...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>132</td>\n      <td>5yxYokipsWlpDCt4Th4VVc</td>\n      <td>547</td>\n      <td>[psychedelic rock, country rock, rock en espanol]</td>\n      <td>[album rock, art rock, classic rock, folk rock...</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data_path = '../data/merged_data.jsonl'\n",
    "data = pd.read_json(merged_data_path, lines=True)\n",
    "\n",
    "data = data.drop(\n",
    "    columns=[\"release_date\", \"key\", \"loudness\",\n",
    "             \"explicit\", \"popularity\", \"duration_ms\", \"danceability\", \"energy\", \"speechiness\",\n",
    "             \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"])\n",
    "\n",
    "data.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_favourite_genres 46\n",
      "unique_genres 1766\n",
      "common genres {'j-pop', 'metal', 'latin', 'regional mexican', 'mandopop', 'brill building pop', 'latin alternative', 'hard rock', 'permanent wave', 'pop', 'new wave pop', 'tropical', 'rock en espanol', 'singer-songwriter', 'turkish pop', 'filmi', 'soul', 'classic rock', 'alternative metal', 'new romantic', 'art rock', 'lounge', 'soft rock', 'vocal jazz', 'europop', 'psychedelic rock', 'rock', 'pop rock', 'mpb', 'ranchera', 'alternative rock', 'c-pop', 'quiet storm', 'hoerspiel', 'new wave', 'latin pop', 'country rock', 'album rock', 'blues rock', 'folk', 'motown', 'latin rock', 'funk', 'mellow gold', 'adult standards', 'dance pop'}\n"
     ]
    }
   ],
   "source": [
    "# unique_favourite_genres = set()\n",
    "# for genres in data['favourite_genres']:\n",
    "#   for genre in genres:\n",
    "#     unique_favourite_genres.add(genre)\n",
    "#\n",
    "# unique_genres = set()\n",
    "# for genres in data['genres']:\n",
    "#   for genre in genres:\n",
    "#     unique_genres.add(genre)\n",
    "#\n",
    "# # show all common genres between all favourite_genres and genres\n",
    "# print(\"unique_favourite_genres\", len(unique_favourite_genres))\n",
    "# print(\"unique_genres\", len(unique_genres))\n",
    "# print(\"common genres\", unique_favourite_genres.intersection(unique_genres))\n",
    "#\n",
    "# # remove genres that are not in favourite_genres\n",
    "# data['genres'] = data['genres'].apply(\n",
    "#     lambda x: [genre for genre in x if genre in unique_favourite_genres])\n",
    "\n",
    "# TODO remove ??? moze niepotrzebne"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Combine genres and favourite_genres\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m all_genres \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[43mdata\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfavourite_genres\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgenres\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# One-hot encode the genres\u001B[39;00m\n\u001B[1;32m      5\u001B[0m mlb \u001B[38;5;241m=\u001B[39m MultiLabelBinarizer()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine genres and favourite_genres\n",
    "all_genres = list(data['favourite_genres'] + data['genres'])\n",
    "\n",
    "# One-hot encode the genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(all_genres)\n",
    "\n",
    "# encoded_all_genres = mlb.fit_transform(all_genres)\n",
    "\n",
    "# Split encoded_genres into genres and favourite_genres\n",
    "# encoded_favourite_genres = encoded_all_genres[:, :len(data['favourite_genres'][0])]\n",
    "# encoded_genres = encoded_all_genres[:, len(data['favourite_genres'][0]):]\n",
    "\n",
    "encoded_favourite_genres = mlb.transform(data['favourite_genres'][0])\n",
    "encoded_genres = mlb.transform(data['genres'][0])\n",
    "\n",
    "# create data frame from data genres, data favourite_genres, encoded genres, encoded favourite_genres\n",
    "df = pd.DataFrame(data={'genres': data['genres'], 'favourite_genres': data['favourite_genres'],\n",
    "                        'encoded_genres': encoded_genres,\n",
    "                        'encoded_favourite_genres': encoded_favourite_genres})\n",
    "\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train skipped 2905\n",
      "X_train not skipped 4999\n",
      "X_train not skipped % 0.6324645748987854\n"
     ]
    }
   ],
   "source": [
    "# TODO uzywac keras tokenizer?\n",
    "\n",
    "# Concatenate the one-hot encoded columns\n",
    "X = np.concatenate([encoded_favourite_genres, encoded_genres], axis=1)\n",
    "\n",
    "# Extract the labels\n",
    "y = data['skipped'].astype(int).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# count skipped and not skipped songs in X_train\n",
    "print(\"X_train skipped\", np.count_nonzero(y_train == 1))\n",
    "print(\"X_train not skipped\", np.count_nonzero(y_train == 0))\n",
    "print(\"X_train not skipped %\", np.count_nonzero(y_train == 0) / len(y_train))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(46 * 2, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(46, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  # model.add(Dense(1000, activation='relu'))\n",
    "  # model.add(Dropout(0.5))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  # model.compile(\n",
    "  #     optimizer=keras.optimizers.Adam(hp.Choice('learning_date', values=[0.5, 0.1, 0.01])),\n",
    "  #     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  model.compile(\n",
    "      optimizer=Adam(),\n",
    "      loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "198/198 - 3s - loss: 0.6480 - accuracy: 0.6250 - val_loss: 0.6256 - val_accuracy: 0.6762 - 3s/epoch - 13ms/step\n",
      "Epoch 2/50\n",
      "198/198 - 1s - loss: 0.6121 - accuracy: 0.6714 - val_loss: 0.6031 - val_accuracy: 0.6818 - 1s/epoch - 6ms/step\n",
      "Epoch 3/50\n",
      "198/198 - 1s - loss: 0.5900 - accuracy: 0.6895 - val_loss: 0.5934 - val_accuracy: 0.6837 - 1s/epoch - 6ms/step\n",
      "Epoch 4/50\n",
      "198/198 - 1s - loss: 0.5819 - accuracy: 0.6853 - val_loss: 0.5891 - val_accuracy: 0.6932 - 1s/epoch - 6ms/step\n",
      "Epoch 5/50\n",
      "198/198 - 1s - loss: 0.5773 - accuracy: 0.6963 - val_loss: 0.5829 - val_accuracy: 0.6901 - 1s/epoch - 6ms/step\n",
      "Epoch 6/50\n",
      "198/198 - 1s - loss: 0.5760 - accuracy: 0.6935 - val_loss: 0.5829 - val_accuracy: 0.6856 - 1s/epoch - 6ms/step\n",
      "Epoch 7/50\n",
      "198/198 - 1s - loss: 0.5739 - accuracy: 0.7003 - val_loss: 0.5801 - val_accuracy: 0.6799 - 1s/epoch - 6ms/step\n",
      "Epoch 8/50\n",
      "198/198 - 1s - loss: 0.5667 - accuracy: 0.7041 - val_loss: 0.5790 - val_accuracy: 0.6970 - 1s/epoch - 6ms/step\n",
      "Epoch 9/50\n",
      "198/198 - 1s - loss: 0.5654 - accuracy: 0.7038 - val_loss: 0.5819 - val_accuracy: 0.6926 - 1s/epoch - 6ms/step\n",
      "Epoch 10/50\n",
      "198/198 - 1s - loss: 0.5664 - accuracy: 0.6994 - val_loss: 0.5798 - val_accuracy: 0.6888 - 1s/epoch - 6ms/step\n",
      "Epoch 11/50\n",
      "198/198 - 1s - loss: 0.5614 - accuracy: 0.7050 - val_loss: 0.5763 - val_accuracy: 0.6869 - 1s/epoch - 6ms/step\n",
      "Epoch 12/50\n",
      "198/198 - 1s - loss: 0.5620 - accuracy: 0.7071 - val_loss: 0.5757 - val_accuracy: 0.6977 - 1s/epoch - 6ms/step\n",
      "Epoch 13/50\n",
      "198/198 - 1s - loss: 0.5600 - accuracy: 0.7025 - val_loss: 0.5764 - val_accuracy: 0.6907 - 1s/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "198/198 - 1s - loss: 0.5567 - accuracy: 0.7055 - val_loss: 0.5756 - val_accuracy: 0.6945 - 1s/epoch - 6ms/step\n",
      "Epoch 15/50\n",
      "198/198 - 1s - loss: 0.5559 - accuracy: 0.7101 - val_loss: 0.5764 - val_accuracy: 0.6882 - 1s/epoch - 6ms/step\n",
      "Epoch 16/50\n",
      "198/198 - 1s - loss: 0.5520 - accuracy: 0.7164 - val_loss: 0.5736 - val_accuracy: 0.6920 - 1s/epoch - 6ms/step\n",
      "Epoch 17/50\n",
      "198/198 - 1s - loss: 0.5545 - accuracy: 0.7139 - val_loss: 0.5739 - val_accuracy: 0.6907 - 1s/epoch - 6ms/step\n",
      "Epoch 18/50\n",
      "198/198 - 1s - loss: 0.5458 - accuracy: 0.7134 - val_loss: 0.5725 - val_accuracy: 0.6894 - 1s/epoch - 6ms/step\n",
      "Epoch 19/50\n",
      "198/198 - 1s - loss: 0.5511 - accuracy: 0.7137 - val_loss: 0.5729 - val_accuracy: 0.6945 - 1s/epoch - 6ms/step\n",
      "Epoch 20/50\n",
      "198/198 - 1s - loss: 0.5480 - accuracy: 0.7139 - val_loss: 0.5753 - val_accuracy: 0.6932 - 1s/epoch - 6ms/step\n",
      "Epoch 21/50\n",
      "198/198 - 1s - loss: 0.5478 - accuracy: 0.7166 - val_loss: 0.5744 - val_accuracy: 0.6850 - 1s/epoch - 6ms/step\n",
      "Epoch 22/50\n",
      "198/198 - 1s - loss: 0.5438 - accuracy: 0.7153 - val_loss: 0.5756 - val_accuracy: 0.6831 - 1s/epoch - 6ms/step\n",
      "Epoch 23/50\n",
      "198/198 - 1s - loss: 0.5441 - accuracy: 0.7147 - val_loss: 0.5728 - val_accuracy: 0.6894 - 1s/epoch - 6ms/step\n",
      "Epoch 24/50\n",
      "198/198 - 1s - loss: 0.5420 - accuracy: 0.7186 - val_loss: 0.5751 - val_accuracy: 0.6818 - 1s/epoch - 6ms/step\n",
      "Epoch 25/50\n",
      "198/198 - 1s - loss: 0.5409 - accuracy: 0.7199 - val_loss: 0.5717 - val_accuracy: 0.6926 - 1s/epoch - 6ms/step\n",
      "Epoch 26/50\n",
      "198/198 - 2s - loss: 0.5400 - accuracy: 0.7199 - val_loss: 0.5730 - val_accuracy: 0.6913 - 2s/epoch - 9ms/step\n",
      "Epoch 27/50\n",
      "198/198 - 1s - loss: 0.5419 - accuracy: 0.7180 - val_loss: 0.5756 - val_accuracy: 0.6882 - 1s/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "198/198 - 1s - loss: 0.5369 - accuracy: 0.7215 - val_loss: 0.5745 - val_accuracy: 0.6863 - 1s/epoch - 6ms/step\n",
      "Epoch 29/50\n",
      "198/198 - 1s - loss: 0.5362 - accuracy: 0.7191 - val_loss: 0.5722 - val_accuracy: 0.6831 - 1s/epoch - 6ms/step\n",
      "Epoch 30/50\n",
      "198/198 - 2s - loss: 0.5362 - accuracy: 0.7147 - val_loss: 0.5755 - val_accuracy: 0.6831 - 2s/epoch - 8ms/step\n",
      "Epoch 31/50\n",
      "198/198 - 1s - loss: 0.5380 - accuracy: 0.7204 - val_loss: 0.5739 - val_accuracy: 0.6850 - 1s/epoch - 7ms/step\n",
      "Epoch 32/50\n",
      "198/198 - 1s - loss: 0.5353 - accuracy: 0.7267 - val_loss: 0.5739 - val_accuracy: 0.6869 - 1s/epoch - 7ms/step\n",
      "Epoch 33/50\n",
      "198/198 - 1s - loss: 0.5353 - accuracy: 0.7220 - val_loss: 0.5733 - val_accuracy: 0.6837 - 1s/epoch - 6ms/step\n",
      "Epoch 34/50\n",
      "198/198 - 1s - loss: 0.5334 - accuracy: 0.7254 - val_loss: 0.5780 - val_accuracy: 0.6882 - 1s/epoch - 6ms/step\n",
      "Epoch 35/50\n",
      "198/198 - 1s - loss: 0.5315 - accuracy: 0.7289 - val_loss: 0.5761 - val_accuracy: 0.6882 - 1s/epoch - 6ms/step\n",
      "Epoch 36/50\n",
      "198/198 - 1s - loss: 0.5327 - accuracy: 0.7262 - val_loss: 0.5740 - val_accuracy: 0.6920 - 1s/epoch - 6ms/step\n",
      "Epoch 37/50\n",
      "198/198 - 1s - loss: 0.5321 - accuracy: 0.7273 - val_loss: 0.5763 - val_accuracy: 0.6926 - 1s/epoch - 6ms/step\n",
      "Epoch 38/50\n",
      "198/198 - 1s - loss: 0.5323 - accuracy: 0.7297 - val_loss: 0.5763 - val_accuracy: 0.6894 - 1s/epoch - 6ms/step\n",
      "Epoch 39/50\n",
      "198/198 - 1s - loss: 0.5320 - accuracy: 0.7266 - val_loss: 0.5734 - val_accuracy: 0.6939 - 1s/epoch - 6ms/step\n",
      "Epoch 40/50\n",
      "198/198 - 1s - loss: 0.5264 - accuracy: 0.7335 - val_loss: 0.5749 - val_accuracy: 0.6926 - 1s/epoch - 6ms/step\n",
      "Epoch 41/50\n",
      "198/198 - 1s - loss: 0.5266 - accuracy: 0.7332 - val_loss: 0.5788 - val_accuracy: 0.6888 - 1s/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "198/198 - 1s - loss: 0.5263 - accuracy: 0.7291 - val_loss: 0.5795 - val_accuracy: 0.6882 - 1s/epoch - 6ms/step\n",
      "Epoch 43/50\n",
      "198/198 - 1s - loss: 0.5251 - accuracy: 0.7275 - val_loss: 0.5766 - val_accuracy: 0.6894 - 1s/epoch - 6ms/step\n",
      "Epoch 44/50\n",
      "198/198 - 1s - loss: 0.5253 - accuracy: 0.7291 - val_loss: 0.5776 - val_accuracy: 0.6907 - 1s/epoch - 6ms/step\n",
      "Epoch 45/50\n",
      "198/198 - 1s - loss: 0.5244 - accuracy: 0.7281 - val_loss: 0.5780 - val_accuracy: 0.6869 - 1s/epoch - 6ms/step\n",
      "Epoch 46/50\n",
      "198/198 - 1s - loss: 0.5259 - accuracy: 0.7357 - val_loss: 0.5775 - val_accuracy: 0.6882 - 1s/epoch - 6ms/step\n",
      "Epoch 47/50\n",
      "198/198 - 1s - loss: 0.5231 - accuracy: 0.7353 - val_loss: 0.5763 - val_accuracy: 0.6869 - 1s/epoch - 6ms/step\n",
      "Epoch 48/50\n",
      "198/198 - 1s - loss: 0.5220 - accuracy: 0.7330 - val_loss: 0.5786 - val_accuracy: 0.6875 - 1s/epoch - 6ms/step\n",
      "Epoch 49/50\n",
      "198/198 - 1s - loss: 0.5223 - accuracy: 0.7327 - val_loss: 0.5779 - val_accuracy: 0.6812 - 1s/epoch - 6ms/step\n",
      "Epoch 50/50\n",
      "198/198 - 1s - loss: 0.5199 - accuracy: 0.7354 - val_loss: 0.5802 - val_accuracy: 0.6863 - 1s/epoch - 6ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "model = build_model(None)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# tuner = keras_tuner.tuners.Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',\n",
    "#     max_epochs=50,\n",
    "#     max_trials=10,\n",
    "#     executions_per_trial=2,\n",
    "#     directory='my_dir')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tuner.search(\n",
    "#     (X_train, y_train),\n",
    "#     validation_data=(X_test, y_test),\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "\n",
    "print(\"TEST\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_classes))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_classes))\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred_classes))\n",
    "\n",
    "print(\"TRAIN\")\n",
    "y_pred = model.predict(X_train)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred_classes))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_train, y_pred_classes))\n",
    "print(\"Classification report:\\n\", classification_report(y_train, y_pred_classes))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Make predictions on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "#\n",
    "# # Print the predicted and actual labels\n",
    "# print(\"Predicted labels:\", y_pred_classes.flatten())\n",
    "# print(\"Actual labels:\", y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # test with new data\n",
    "#\n",
    "# new_data = [\n",
    "#   ([\"dominican pop\", \"merengue\", \"merengue tipico\", \"tropical\"],\n",
    "#    [\"blues rock\", \"country rock\", \"lounge\"])\n",
    "# ]\n",
    "#\n",
    "# labels = [\n",
    "#\n",
    "# ]\n",
    "#\n",
    "# new_df = pd.DataFrame(new_data, columns=[\"genres\", \"favourite_genres\"])\n",
    "#\n",
    "# # Combine genres and favourite_genres\n",
    "# all_new_genres = list(new_df['genres'] + new_df['favourite_genres'])\n",
    "#\n",
    "# # One-hot encode the genres using the previously fit MultiLabelBinarizer (mlb)\n",
    "# encoded_new_genres = mlb.transform(all_new_genres)\n",
    "#\n",
    "# # Split encoded_new_genres into genres and favourite_genres\n",
    "# encoded_new_genres1 = encoded_new_genres[:, :len(new_df['genres'][0])]\n",
    "# encoded_new_genres2 = encoded_new_genres[:, len(new_df['genres'][0]):]\n",
    "#\n",
    "# # Concatenate the one-hot encoded columns\n",
    "# X_new = np.concatenate([encoded_new_genres1, encoded_new_genres2], axis=1)\n",
    "#\n",
    "# y_new_pred = model.predict(X_new)\n",
    "# y_new_pred_classes = (y_new_pred > 0.5).astype(int)\n",
    "#\n",
    "# # Print the predicted labels\n",
    "# print(\"Predicted labels:\", y_new_pred_classes.flatten())\n",
    "# print(\"Actual labels:\", labels)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ium",
   "language": "python",
   "display_name": "Python 3.10 (IUM)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
